[
  {
    "objectID": "repositories.html",
    "href": "repositories.html",
    "title": "Source Code Repositories",
    "section": "",
    "text": "unicef/magasin\n\nPrimary respository, holds main components, setup and admin scripts, as well as documentation"
  },
  {
    "objectID": "repositories.html#magasin-main-project",
    "href": "repositories.html#magasin-main-project",
    "title": "Source Code Repositories",
    "section": "",
    "text": "unicef/magasin\n\nPrimary respository, holds main components, setup and admin scripts, as well as documentation"
  },
  {
    "objectID": "repositories.html#libraries-and-utilities",
    "href": "repositories.html#libraries-and-utilities",
    "title": "Source Code Repositories",
    "section": "2 Libraries and utilities",
    "text": "2 Libraries and utilities\n\nunicef/superset-dashboard-cloner Scripts for cloning Apache Superset dashboards to create copies that use different data sources."
  },
  {
    "objectID": "repositories.html#custom-helm-charts",
    "href": "repositories.html#custom-helm-charts",
    "title": "Source Code Repositories",
    "section": "3 Custom Helm charts",
    "text": "3 Custom Helm charts\n\nunicef/magasin-drill A helm chart that allows setting up Apache Drill in cluster mode together with Zookeeper in a Kubernetes cluster."
  },
  {
    "objectID": "admin-guides/kubernetes.html",
    "href": "admin-guides/kubernetes.html",
    "title": "Kubernetes guide",
    "section": "",
    "text": "A quick guide for a magasin cluster operator."
  },
  {
    "objectID": "admin-guides/kubernetes.html#general-kubernetes",
    "href": "admin-guides/kubernetes.html#general-kubernetes",
    "title": "Kubernetes guide",
    "section": "0.1 General kubernetes",
    "text": "0.1 General kubernetes\nMagasin is installed in a kubernetes cluster, in this chapter you have a summary of common kubernetes commands.\n\n0.1.1 Get the list of namespaces.\nKubernetes resources are kept in namespaces. A namespace is like a folder but where you can keep kubernetes resources such as pods, secrets, etc.\nkubectl get namespaces\n\n\n0.1.2 Get the list of resources\nTo get the resources available within a particular namespace:\nkubectl get &lt;resource-type&gt; --namespace &lt;namespace&gt;\nwhere resource-type is one of\n\npod,\nsecret,\nservice,\npv (persistent volume),\npvc (persitstent volume claim),\n\nExamples:\nkubectl get pods --namespace magasin-drill\nkubectl get secrets --namespace magasin-drill\nOr alternatively\nkubectl get pods -n magasin-drill\nIf --namespace is not set, it displays the resources of the default namespace.\n\n\n\n\n\n\nTip: to change the default namespace use the command kubectl config set-context --current --namespace=&lt;namespace&gt; to update set the default namespace so you don’t need to set --namespace &lt;namespace&gt; on each command\n\n\n\n\n\n0.1.3 Get the list of secrets\nA secret is a type of resource in kubernetes that allows you to keep configuration information such as usernames, tokens, database names, etc.\nTo get the list of secrets of a particular namespace\nkubectl get secret --namespace magasin-superset\nIn magasin, these are some secrets that contain interesting information for the admin are (format application / secret-name):\n\nmagasin-dagster- / TODO-XXXX:\nmagasin-drill / drill-storage-plugin-secret: contains the initial setup of the storage.\nmagasin-superset / superset-env: Contains the environment variables of the application.\nmagasin-daskhub / hub-env. Contains the environment variables of the application.\n\nOne secret may contain more than variable. To view the different variables (items) of a particular secret:\nkubectl describe secret superset-env --namespace magasin-superset\nwhere superset-env is the name of the secret."
  },
  {
    "objectID": "admin-guides/drill.html",
    "href": "admin-guides/drill.html",
    "title": "Drill guide",
    "section": "",
    "text": "A quick guide for managing magasin’s Apache Drill instance."
  },
  {
    "objectID": "admin-guides/drill.html#apache-drill-options-and-tuning",
    "href": "admin-guides/drill.html#apache-drill-options-and-tuning",
    "title": "Drill guide",
    "section": "1 Apache Drill options and tuning",
    "text": "1 Apache Drill options and tuning\nThe following option values are recommended for Apache Drill’s use in the Magasin context:\nplanner.width.max_per_node = 3\nplanner.width.max_per_query = 12\nstore.parquet.reader.int96_as_timestamp = true\ndrill.exec.http.rest.errors.verbose = true\nexec.errors.verbose = true\nexec.queue.enable = true\nexec.queue.large = 2\nexec.queue.small = 10"
  },
  {
    "objectID": "admin-guides/drill.html#example-apache-drill-storage-plugin-for-azure-blob-storage",
    "href": "admin-guides/drill.html#example-apache-drill-storage-plugin-for-azure-blob-storage",
    "title": "Drill guide",
    "section": "2 Example Apache Drill storage plugin for Azure blob storage",
    "text": "2 Example Apache Drill storage plugin for Azure blob storage\nExample Apache Drill storage plugin configuration for a specific Azure blob storage container (unicef-magasin-dev) in a given Azure blob storage account (sauniwebsaksio).\nThe Apache Drill Azure Blob Storage Plugin must be present in $DRILL_HOME/jars/3rdparty. Note the Azure authentication key has been redacted in the below example.\nThis configuration is read-only and specifies support for certain file formats within two directory locations (/profiles and /datasets) within the storage container.\nFor production configuration, file formats should be restricted to the minimal set of expected formats. Multiple storage plugin instances can be configured in a single Apache Drill instance so separate Azure blob storage accounts and containers may be separately configured and queried.\n{\n  \"name\" : \"storage_account_blob_container\",\n  \"config\" : {\n    \"type\" : \"file\",\n    \"connection\" : \"wasbs://blob_container@storage_account.blob.core.windows.net\",\n    \"config\" : {\n      \"fs.azure.account.key.storage_account.blob.core.windows.net\" : \"*******\"\n    },\n    \"workspaces\" : {\n      \"profiles\" : {\n        \"location\" : \"/profiles\",\n        \"writable\" : false,\n        \"defaultInputFormat\" : null,\n        \"allowAccessOutsideWorkspace\" : false\n      },\n      \"datasets\" : {\n        \"location\" : \"/datasets\",\n        \"writable\" : false,\n        \"defaultInputFormat\" : null,\n        \"allowAccessOutsideWorkspace\" : false\n      }\n    },\n    \"formats\" : {\n      \"image\" : {\n        \"type\" : \"image\",\n        \"extensions\" : [ \"jpg\", \"jpeg\", \"jpe\", \"tif\", \"tiff\", \"dng\", \"psd\", \"png\", \"bmp\", \"gif\", \"ico\", \"pcx\", \"wav\", \"wave\", \"avi\", \"webp\", \"mov\", \"mp4\", \"m4a\", \"m4p\", \"m4b\", \"m4r\", \"m4v\", \"3gp\", \"3g2\", \"eps\", \"epsf\", \"epsi\", \"ai\", \"arw\", \"crw\", \"cr2\", \"nef\", \"orf\", \"raf\", \"rw2\", \"rwl\", \"srw\", \"x3f\" ],\n        \"fileSystemMetadata\" : true,\n        \"descriptive\" : true\n      },\n      \"parquet\" : {\n        \"type\" : \"parquet\"\n      },\n      \"avro\" : {\n        \"type\" : \"avro\",\n        \"extensions\" : [ \"avro\" ]\n      },\n      \"json\" : {\n        \"type\" : \"json\",\n        \"extensions\" : [ \"json\" ]\n      },\n      \"sequencefile\" : {\n        \"type\" : \"sequencefile\",\n        \"extensions\" : [ \"seq\" ]\n      },\n      \"tsv\" : {\n        \"type\" : \"text\",\n        \"extensions\" : [ \"tsv\" ],\n        \"fieldDelimiter\" : \"\\t\"\n      },\n      \"csvh\" : {\n        \"type\" : \"text\",\n        \"extensions\" : [ \"csvh\" ],\n        \"extractHeader\" : true\n      },\n      \"csv\" : {\n        \"type\" : \"text\",\n        \"extensions\" : [ \"csv\" ]\n      },\n      \"psv\" : {\n        \"type\" : \"text\",\n        \"extensions\" : [ \"tbl\" ],\n        \"fieldDelimiter\" : \"|\"\n      },\n      \"pcap\" : {\n        \"type\" : \"pcap\",\n        \"extensions\" : [ \"pcap\" ]\n      },\n      \"httpd\" : {\n        \"type\" : \"httpd\",\n        \"extensions\" : [ \"httpd\" ],\n        \"logFormat\" : \"%h %t \\\"%r\\\" %&gt;s %b \\\"%{Referer}i\\\"\"\n      }\n    },\n    \"enabled\" : true\n  }\n}"
  },
  {
    "objectID": "admin-guides/drill.html#backup-the-drill-storage-accounts-database",
    "href": "admin-guides/drill.html#backup-the-drill-storage-accounts-database",
    "title": "Drill guide",
    "section": "3 Backup the drill storage accounts database",
    "text": "3 Backup the drill storage accounts database\nApache Drill keeps a set of connections to storage accounts (S3 buckets, azure blobs, MinIO accounts…). The storage accounts is where the actual data is stored.\nHowever, in magasin given that Drill is setup in a distributed mode, the configuration of these connections is kept in zookeeper which is used to sync the storage configuration among the different instances of Drill. So, to backup the storage accounts we need to backup the zookeeper database which is a plain database.\n\nLaunch a terminal within the zookeeper pod (zk-0).\nkubectl exec zk-0 --namespace magasin-drill -ti -- /bin/bash\nCompress the files using tar, and leave the shell\ntar -czvf /tmp/zk.tgz /var/lib/zookeeper/*\nexit\nSave the file into our local filesystem\nkubectl cp magasin-drill/zk-0:/tmp/zk.tgz ./zk.tgz\nDelete the .tgz file of the pod\nkubectl exec zk-0 --namespace magasin-drill -- rm tmp/zk.tgz"
  },
  {
    "objectID": "admin-guides/drill.html#restore-the-drill-storage-configuration",
    "href": "admin-guides/drill.html#restore-the-drill-storage-configuration",
    "title": "Drill guide",
    "section": "4 Restore the Drill storage configuration",
    "text": "4 Restore the Drill storage configuration\nTo restore the backup created in the previous section:\n\nCopy the backup to the cluster\n kubectl cp ./zk.tgz magasin-drill/zk-0:/tmp/zk.tgz \nLaunch the shell in the zookeeper pod\nkubectl exec zk-0 --namespace magasin-drill -ti -- /bin/bash\nUnzip\ntar -xzvf /tmp/zk.tgz \nKill the java process\nps -ax \n\nPID TTY      STAT   TIME COMMAND\n1 ?          Ss     0:00 /rosetta/rosetta /usr/bin/bash /usr/bin/start.sh\n25 ?         Sl     0:03 /rosetta/rosetta /usr/bin/java -Dzookeeper.log.dir=/opt/zookeeper/bin/../logs -Dzookeeper.log.file=zookeeper-\n116 pts/0    Ss     0:00 /rosetta/rosetta /bin/bash\n148 ?        S      0:00 /rosetta/rosetta /usr/bin/sleep 5\n149 pts/0    R+     0:00 /usr/bin/ps -ax\n\n# Where -9 is to force kill and 25 is the process number (PID)\nkill -9 25\nRelaunch the initalizer script in background (&), then exit\n/usr/bin/start.sh &\nexit"
  },
  {
    "objectID": "get-started/tutorial-overview.html",
    "href": "get-started/tutorial-overview.html",
    "title": "Tutorial overview",
    "section": "",
    "text": "We wil follow this process simple process that is similar to what you may do in the real world:\nflowchart LR\n  A(1.Manual analysis using a \\nJupyter Notebook) --&gt; B(2.Automate data ingestion with\\n Dagster)\n  B --&gt; C(3.Create a Dashboard with\\n Superset)"
  },
  {
    "objectID": "get-started/tutorial-overview.html#step-1-initial-data-analysis-in-a-jupyter-notebook",
    "href": "get-started/tutorial-overview.html#step-1-initial-data-analysis-in-a-jupyter-notebook",
    "title": "Tutorial overview",
    "section": "0.1 Step 1: Initial data analysis in a Jupyter Notebook",
    "text": "0.1 Step 1: Initial data analysis in a Jupyter Notebook\nGenerally, before you start automating any process of regularly using some data you want to do some initial research and analysis. For that, Jupyter Notebook is a great tool. It allows you to run small pieces of code (python or R) individually and interactively in a user friendly interface.\nMagasin comes with the Jupyter Hub, which allows different users to run jupyter notebooks on the cloud without the need of installing anything on their own system.\nIn this case, we are going play around with one simple API, the Digital Public Goods Alliance (DPGA) API.\nDigital Public Goods (DPGs) are public goods in the form of software, data sets, AI models, standards or content that are generally free cultural works and have an intention to contribute to sustainable national and international digital development. Several international agencies, including UNICEF and UNDP, are exploring DPGs as a possible solution to address the issue of digital inclusion, particularly for children in emerging economies.\nFrom the data that we extract from the API, we are going to query the data to get some insights how many DPGs are available, in which countries are being deployed, where are they developed, what types of licenses are more popular…\nJupyter Notebooks are excellent for an initial analysis, but there are many use cases in which you may to repeat the same process periodically. In our case, we may want to track if new DPGs are comming or what are the trends in terms of deployments. This is when the next steps of the process are useful."
  },
  {
    "objectID": "get-started/tutorial-overview.html#step-2-automate-data-ingestion-with-dagster",
    "href": "get-started/tutorial-overview.html#step-2-automate-data-ingestion-with-dagster",
    "title": "Tutorial overview",
    "section": "0.2 Step 2: Automate data ingestion with Dagster",
    "text": "0.2 Step 2: Automate data ingestion with Dagster\nAutomating is great, it frees us on needing to do repeated work. In our example, we will automate the ingestion and transformation of the DPGA API data. The data as it is served by the API is not fully ready to be analyzed, we need to perform some transformations to leave it ready to be displayed.\nWe did the heavy-lifting interactively with the first analysis and now we just need to convert it to dagster pipeline.\nDagster is what is called a pipeline orchestrator, which basically helps us to manage the ingestion of data from multiple data sources. The advantages of using a framework like dagster is that it will allow us to scale and to monitor.\nWhat usually happends when you start automating stuff is that:\n\nIt starts simple but with time, you add more and more data sources that have dependencies between them. If it is not done properly you end up with a mess. Dagster provides us a framework to prevent that.\nIt eventually it may break. Data sources may change with time and with that your pipelines break. For instance, DPGA may change the API and our automation will fail. Dagster allows you to monitor and to\n\nThe cool thing of Dagster, it that it uses Python as programming language too, so most of the work we did for the"
  },
  {
    "objectID": "get-started/tutorial-overview.html#step-3-create-a-dashboard-with-superset",
    "href": "get-started/tutorial-overview.html#step-3-create-a-dashboard-with-superset",
    "title": "Tutorial overview",
    "section": "0.3 Step 3: Create a dashboard with Superset",
    "text": "0.3 Step 3: Create a dashboard with Superset\nDagster allows you to gather the data and transform it to something that can be displayed, but it does not come with those visualization capabilities. For that we need a tool that has many graphs, what is called a business intelligence dashboard tool.\nMagasin comes with Apache Superset, which allows you to create dashboard with many graphs.\nIn this last step, we will create a dashboard that will display the graphs.\nAgain, we did the heavy lifting on the first step with the notebook so now we just need to play with the user interface of superset."
  },
  {
    "objectID": "get-started/create-dashboard.html",
    "href": "get-started/create-dashboard.html",
    "title": "Step 3: Create a dashboard",
    "section": "",
    "text": "Step 3: Create a Dashboard"
  },
  {
    "objectID": "deployment.html",
    "href": "deployment.html",
    "title": "Production deployment",
    "section": "",
    "text": "In this guide we will explain everything you need to do in order to have a production environment.\nWe are working on this guide. Please, come back soon"
  },
  {
    "objectID": "development.html",
    "href": "development.html",
    "title": "Development",
    "section": "",
    "text": "Some recipies that will help you to set things up when. This content is specially focused on the common tasks of magasin contributors and maintainers."
  },
  {
    "objectID": "development.html#how-can-i-update-magasins-repo-helm-charts",
    "href": "development.html#how-can-i-update-magasins-repo-helm-charts",
    "title": "Development",
    "section": "1 How can I update magasin’s repo helm charts?",
    "text": "1 How can I update magasin’s repo helm charts?\nGenerally, magasin helm charts are just a copy of the “official” repositories. So, what we do is we get a copy of these externally developed charts and create a copy. Then we test interoperability the magasin tools are working and release the package. In some sort of way we follow the\nCurrently, we have a script that updates the folder /helm that contains magasin’s charts. It gets them from from other helm repos and copies them to the helm/ folder. The script is in /dev-scripts/update-helm-charts.sh.\n\nClone the repo\ngit clone https://github.com/unicef/magasin\nUpdate the versions editing /dev-sripts/update-helm-charts.sh\nFor now, it has the version numbers of the helm charts harcoded. So, you have to modify it whenever you want to update the charts. The script file is documented and should guide you on how to proceed.\nRun the script."
  },
  {
    "objectID": "development.html#how-can-i-test-changes-on-helm-charts",
    "href": "development.html#how-can-i-test-changes-on-helm-charts",
    "title": "Development",
    "section": "2 How can I test changes on helm charts?",
    "text": "2 How can I test changes on helm charts?\nIf you have updated or modified the helm charts of the /helm folder, you may want to test them before performing a release.\nThe easiest way is to run a helm repo in your local machine, which basically is an HTTP server that has the helm charts packaged as .tgz files and a metadata file called index.yaml. Magasin includes a script that allows you to do this.\nLet’s wee what are the steps:\n\nClone the repo\ngit clone https://github.com/unicef/magasin\nMake the changes on the helm charts within the folder /helm\nServe the helm charts in a local repo.\nGo to the dev-scripts and run the local-helm-repo.sh script.\n cd dev-scripts\n ./local-helm-repo.should\nThis script packages the helm charts in the /helm folder and launches an HTTP server that points to the folder /_helm-repo/ in the root folder of the repo in the port 8000. You can check it is running by opening a browser http://localhost:8000/index.yaml\nInstall magasin.\nGo to the scripts/installer folder and run the magasin installer specifying the url of the local repository\n cd magasin/scripts/installer\n ./install-magasin.sh -u http://localhost:8000\n\n\n2.1 What if I only want to update or install one single component?\nIn the previous section you had to install all the charts of magasin it assumed that you did not have an instance already running, but that you already have one instance and you want to update just one chart. To do that you can run the helm command:\nAssuming you are in the root folder of the repo and you want to upgrade Apache Drill (helm/drill)\nIf you are installing the chart for the first time:\nhelm install drill ./helm/drill/  --namespace magasin-drill --create-namespace\nNote that the namespace should match the realm you’re working on. We’re asssuming the starndard namespace\nIf you’re updating the helm chart\nhelm upgrade drill ../../helm/drill/  --namespace magasin-drill\nIn both cases you can also add custom values by adding -f &lt;path-to-my-values-yaml-file&gt;. Example:\nhelm upgrade drill ../../helm/drill/  --namespace magasin-drill -f drill.yaml\nNote that using this method, you don’t need to run the local helm repo script."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "magasin is the cloud native open-source end-to-end data platform\n\n\nmagasin enables organizations to perform of automatic data ingestion, storage, analysis, ML/AI compute and visualization at scale\n\n\n\nGet Started"
  },
  {
    "objectID": "install/advanced.html",
    "href": "install/advanced.html",
    "title": "Advanced installation",
    "section": "",
    "text": "This page provides further information that allows you to better understand how is magasin installed and how you can customize the default installation\nMagasin includes an installer script that sets up all the necessary dependencies on your computer, enabling the seamless setup of within the Kubernetes cluster."
  },
  {
    "objectID": "install/advanced.html#understanding-the-installer-script",
    "href": "install/advanced.html#understanding-the-installer-script",
    "title": "Advanced installation",
    "section": "1 Understanding the installer script",
    "text": "1 Understanding the installer script\nTo install magasin, the most straight forward way is to use the installer. This script, install-magasin.sh script is used for both MacOS and Debian like GNU/Linux and performs two tasks:\n\nThe first one is to install all the pre-requisites required in your computer in order to be able to deploy magasin in a kubernetes cluster\nThe second one is to deploy magasin in the preselected kubernetes cluster.\n\nThese are the steps the script takes:\n\nCheck if all the dependencies are already installed (namely kubectl, helm and python) and if any is missing it will prompt you to install it. It uses the recommended setup describe on the package for the Debian GNU/Linux like system. or in MacOS)\nRun the helm command install the different components of magasin in the kubernetes cluster. Note that each component will be installed in a different namespace within the cluster."
  },
  {
    "objectID": "install/advanced.html#magasin-realms",
    "href": "install/advanced.html#magasin-realms",
    "title": "Advanced installation",
    "section": "2 Magasin realms",
    "text": "2 Magasin realms\nCompared with most of applications that are distributed using helm charts, instead of defining one unique chart that includes all the dependencies, magasin defines an indepenedent chart for each component. This results in loosely-coupled architecture which allows you to only setup and use the components that fit your organizational needs.\nWhen deploying a containerized application in Kubernetes using Helm charts, typically, a singular chart is defined. This chart can actually be composed of multiple sub-charts, which allows for a hierarchical structure within the deployment. This setup often leads to a well-defined architecture, beneficial for many scenarios, as it simplifies management and deployment.\nGiven that magasin setup uses independent charts, there is a need to setup at least one connection between these independent charts. This is done through a consistent naming convention which defines the concept of magasin realms.\nA realm is a way of linking independent helm charts through namespaces that follow consistent naming convention: --.\nBy convention, the default realm only has the prefix magasin and no postfix. For example, the namespace for the component drill that belongs to the default realm is magasin-drill.\nThe realm name is the concatenation of the prefix and the postfix, both separated by an hyphen - (if the postfix is not empty). Examples:\n\n\n\n\n\n\n\n\n\nRealm\nRealm prefix\nRealm postfix\nComponent “drill” namespace\n\n\n\n\nmagasin\nmagasin\n-\nmagasin-drill\n\n\nmagasin-dev\nmagasin\ndev\nmagasin-drill-dev\n\n\nmagasin-new-version-dev\nmagasin-new-version\ndev\nmagasin-new-version-drill-dev\n\n\n-dev\n-\ndev\ndrill-dev\n\n\n\nA realm prefix can have - as part of its names, but not the realm postfix. In the realm name, the last - specifies the start of the postfix.\nIf a realm name starts with - it only has a postfix.\nThe concept of realms allows us setup supporting tools that can make use of two independent helm charts"
  },
  {
    "objectID": "install/advanced.html#advanced-use-of-the-magasin-installer",
    "href": "install/advanced.html#advanced-use-of-the-magasin-installer",
    "title": "Advanced installation",
    "section": "3 Advanced use of the magasin-installer",
    "text": "3 Advanced use of the magasin-installer\nYou can check the options.\n ./install-magasin.sh -h\nUsage: install-magasin.sh [-y] [-c] [-r realm_prefix-realm_postfix (magasin)] [-f values_folder (./)] [-d] [-h]\n\nThis script checks dependencies and installs magasin components\nEach component is installed within its own namespace.\n\nOptions:\n  -y  Skip prompting questions during installation\n  -c  Only check if all pre-requisites are installed in the local machine.\n  -r  Realm prefix and suffix (default: magasin). Prefix and suffix are separated by '-'.\n        If more than one '-', the last one will be used as separator.\n        The realm 'magasin-new-dev' will set 'magasin-new' as prefix and 'dev' as suffix.\n  -f  Folder with custom values.yaml files (default: ./).\n        Files within the folder shall have the same name as the component. Example:\n        drill.yaml, dagster.yaml, superset.yaml, daskhub.yaml\n  -u  URL/path to the magasin's helm repository (default: https://unicef.github.io/magasin/)\n      \n  -d  Enable debug mode (displays all commands run).\n  -h  Display this help message and exit.\n \nExamples of usage:\n\nOnly check if all the required components are installed\n  install-magasin.sh -c \nSetup the realm test-. Will use test- as namespace. Setup the realm magasin-dev. Will use magasin--dev as namespaces\n  install-magasin.sh -r test"
  },
  {
    "objectID": "install/advanced.html#customizing-the-setup-of-each-component",
    "href": "install/advanced.html#customizing-the-setup-of-each-component",
    "title": "Advanced installation",
    "section": "4 Customizing the setup of each component",
    "text": "4 Customizing the setup of each component\nHelm charts allow you to customize some parameters such as the number of replicas, image version to load, startup script parameters, authentication schemas, etc. In order to do that you can create what is called a values file.\nThe installer allows you use custom values file for each component. By default it searches in the current working directory (./) for files that have the name &lt;component&gt;.yaml. That is dagster.yaml, drill.yaml, superset.yaml, daskhub.yaml.\nFor example, by default the Apache Drill helm chart launches two replicas of its main server. You can change the number of replicas by changing the drill.count value you can create the file drill.yaml with the following contents:\n# drill.yaml\ndrill:\n  count: 1\nNow, if you run the installer in the same folder where you stored the drill.yaml file:\ninstall-magasin.sh\nYou’ll see in the logs something like:\n...\n i Installing magasin/drill in the namespace magasin-drill.\n ✓ Custom values file for drill exists (./drill.yaml)\n i helm install drill magasin/drill -f ./drill.yaml --namespace magasin-drill --create-namespace \n...\nBelow you have the default values files and the corresponding custom file name:\n\nDagster default values. To overwrite them create dagster.yaml\nApache Drill default values. To overwrite them create drill.yaml\nDaskhub default values. To overwrite them create daskhub.yaml\n9Superset default values](https://github.com/unicef/magasin/blob/main/helm/superset/values.yaml). To overwrite them create superset.yaml\n\n\n4.1 Setting up different values for different environments\nWhereas for a testing environment you may want to have some values, such as setting to 1 number of replicas of the drill server, for your production environment you may want to have 3 replicas. The -f &lt;folder-path&gt; option allows you to select the folder where you have the customized values the specific environment.\nGiven the below structure\n  /\n  |- dev\n  |  |- drill.yaml\n  |\n  |- prd\n     |- drill.yaml\n     |- dagster.yaml\n     |- superset.yaml\nNow, you can deploy magasin for development running:\ninstall-magasin.sh -r magasin-dev -f ./dev\nAnd the production environment by running:\ninstall-magasin.sh -r magasin-prd -f ./prd\nThanks to the concept of realms, you can have both instances even in the same cluster."
  },
  {
    "objectID": "install/troubleshooting.html",
    "href": "install/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Please check the questions below, if you cannot find the answer you can try asking to magasin’s community within our discussions forum.\n\n\nIn case that you run kubectl cluster-info and it is not pointing to the kubernetes cluster you want to use for magasin you can use the following commands:\n\nGet the list different contexts available\nkubectl config get-contexts\nWhich may output something like: ```sh CURRENT NAME CLUSTER AUTHINFO NAMESPACE\n       docker-desktop              docker-desktop            docker-desktop                                        default\n\n    magasin                     docker-desktop            docker-desktop                                        default\n    magasin-dev                 docker-desktop            docker-desktop                                        default\n    minikube                    minikube                  minikube                                              default\n```\n\nUse the correct cluster (i.e. context). Currently, the context magasin is selected, but to use magasin-dev you can run this command:\nkubectl config use-context magasin-dev"
  },
  {
    "objectID": "install/troubleshooting.html#how-can-i-set-the-kubernetes-correct-cluster-in-kubectl",
    "href": "install/troubleshooting.html#how-can-i-set-the-kubernetes-correct-cluster-in-kubectl",
    "title": "Troubleshooting",
    "section": "",
    "text": "In case that you run kubectl cluster-info and it is not pointing to the kubernetes cluster you want to use for magasin you can use the following commands:\n\nGet the list different contexts available\nkubectl config get-contexts\nWhich may output something like: ```sh CURRENT NAME CLUSTER AUTHINFO NAMESPACE\n       docker-desktop              docker-desktop            docker-desktop                                        default\n\n    magasin                     docker-desktop            docker-desktop                                        default\n    magasin-dev                 docker-desktop            docker-desktop                                        default\n    minikube                    minikube                  minikube                                              default\n```\n\nUse the correct cluster (i.e. context). Currently, the context magasin is selected, but to use magasin-dev you can run this command:\nkubectl config use-context magasin-dev"
  },
  {
    "objectID": "install/index.html",
    "href": "install/index.html",
    "title": "Installation",
    "section": "",
    "text": "Magasin is an scalable end-to-end data platform based on open-source components that is natively run in a kubernetes cluster.\nKubernetes is a container orchestration system designed to automate the deployment, scaling, and management of containerized applications. It is an integral part of services offered by major cloud providers. Kubernetes, being open source, can also be set up on-premises. For testing purposes, it is even possible to install it on a desktop computer.\nMagasin uses Kubernetes in combination with Helm, a package manager for Kubernetes applications. Helm is the equivalent to apt, pip, npm, pacman, snap, conda.. Using Helm, users specify the configuration of required Kubernetes resources to deploy magasin through a values file or command-line overrides. A package in helm is called chart.\nMagasin includes an installer script that sets up all the necessary dependencies on your computer, enabling the seamless setup of within the Kubernetes cluster.\n\n1 Deploy magasin using the installer (beta)\nOnce you have access to a kubernetes cluster, you can use the installer to setup magasin in that cluster.\nThe major goal of this installer is to ease the setup of some tools that need to be installed in your computer, and then deploy magasin in the kubernetes cluster.\n\n\n\n\n\n\nWarning\n\n\n\nIt is highly recommended to take a look at the install script before running it will install several components on your system.\nYou should run curl-bashing (curl piped with bash/zsh) only on providers that you trust. If you’re not confortable with this approach, proceed with the manual installation.\n\n\n\nDeploying magasin from a Debian/Like GNU/Linux computer\ncurl -X https://unicef.github.io/magasin/install-magasin.sh | bash\nDebian like distributions are Ubuntu, Raspbian, Kali Linux, etc. They use apt-get as package manager.\nDeploying magasin from a MacOS computer\ncurl -X https://unicef.github.io/magasin/install-magasin.sh | zsh\n\nIn both cases, you need a user that can run sudo.\nThe installer is in beta, in case it fails check the troubleshooting section and, if the problems persist, try the manual installation\n\nDeploying magasin from a Windows computer Please use the manual installation\n\n\n\n2 What’s next?\nCongratulations, now you have a magasin instance running on your kubernetes cluster. This is just the beggining of an interesting journey. Now you can:\n\nTake a look at magasin tutorial and start using magasin\nLearn about magasin’s architecture. Understand the different components and the underlying technology.\nLearn more about advanced installation. The steps showed in this page were the standard ways of installing magasin, but you can customize much more your setup."
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "Magasin is an scalable end-to-end data platform based on open-source components that is natively run in a kubernetes cluster.\nBy end-to-end it means from the process of enabling the ingestion from multiple data sources, the storage in a cloud filesystems as well as enabling the processes of analyzing and visualizing.\nKubernetes is a container orchestration system designed to automate the deployment, scaling, and management of containerized applications. It is an integral part of services offered by major cloud providers. Kubernetes, being open source, can also be set up on-premises. For testing purposes, it is even possible to install it on a desktop computer.\nMagasin uses Kubernetes in combination with Helm, a package manager for Kubernetes applications. Helm is the equivalent to apt, pip, npm, pacman, snap, conda.. Using Helm, users specify the configuration of required Kubernetes resources to deploy magasin through a values file or command-line overrides. A package in helm is called chart.\nA fundamental contrast between magasin and other helm-based Kubernetes applications lies in their architectural approach. Typically, an application is characterized by a sole root helm chart governing all deployment rules. However, in magasin, each component operates as an autonomous helm chart. This design choice enables the establishment of a loosely-coupled architecture among its components. Rather than mandating a rigid structure for the entire architecture, magasin embraces a more open and adaptable approach, fostering flexibility in component selection and integration.\nThe core components of magasin are independent mature open source projects that support."
  },
  {
    "objectID": "architecture.html#kubernetes-containerization",
    "href": "architecture.html#kubernetes-containerization",
    "title": "Architecture",
    "section": "1.1 Kubernetes containerization",
    "text": "1.1 Kubernetes containerization\nKubernetes is a container orchestration system designed to automate the deployment, scaling, and management of containerized applications. It is an integral part of services offered by major cloud providers. Kubernetes, being open source, can also be set up on-premises. For testing purposes, it is even possible to install it on a desktop computer."
  },
  {
    "objectID": "architecture.html#helm-charts",
    "href": "architecture.html#helm-charts",
    "title": "Architecture",
    "section": "1.2 Helm charts",
    "text": "1.2 Helm charts\nMagasin uses Kubernetes in combination with Helm, a package manager for Kubernetes applications. Helm is the equivalent to apt, pip, npm, pacman, snap, conda, etc. Using Helm, users specify the configuration of required Kubernetes resources to deploy magasin through a values file or command-line overrides. A package in helm is called chart.\nA fundamental contrast between magasin and other helm-based Kubernetes applications lies in their architectural approach. Typically, an application is characterized by a sole root helm chart governing all deployment rules. However, in magasin, each component operates as an autonomous helm chart. This design choice enables the establishment of a loosely-coupled architecture among its components. Rather than mandating a rigid structure for the entire architecture, magasin embraces a more open and adaptable approach, fostering flexibility in component selection and integration."
  },
  {
    "objectID": "architecture.html#ingestion-dagster",
    "href": "architecture.html#ingestion-dagster",
    "title": "Architecture",
    "section": "2.1 Ingestion: Dagster",
    "text": "2.1 Ingestion: Dagster\nThe Dagster framework is the primary tool for orchestration of data pipelines for ingestion, transformation, analysis, and machine learning. Each pipeline is isolated and encapsulated, so different tasks may utilize different versions of the same library, for example, and each pipeline run is executed in a short-lived pod on a Kubernetes cluster.\n\n2.1.1 Dagit\nDagster’s Dagit UI provides visibility of pipelines’ tasks, scheduling, run status, materialized assets, resources, and modes."
  },
  {
    "objectID": "architecture.html#cloud-storage-minio",
    "href": "architecture.html#cloud-storage-minio",
    "title": "Architecture",
    "section": "2.2 Cloud storage: MinIO",
    "text": "2.2 Cloud storage: MinIO\nTODO"
  },
  {
    "objectID": "architecture.html#query-engine-apache-drill",
    "href": "architecture.html#query-engine-apache-drill",
    "title": "Architecture",
    "section": "2.3 Query engine: Apache Drill",
    "text": "2.3 Query engine: Apache Drill\nApache Drill is an open-source, schema-free query engine that provides a SQL interface to a wide range of non-relational datastores, such as NoSQL databases and collections of files such as JSON, CSV, ESRI shapefiles, SPSS & SAS formats, Parquet, and others.\nWhile data marts for specific business functions or locations traditionally require hosting and maintenance of a relational database on a server or virtual machine, Apache Drill enables comparable functionality without need for running and hosting a database or maintaining schema changes from source systems over time.\nInstead, a Dagster ingestion and transformation pipeline stores an ‘analyst-ready’ dataset that Apache Drill can query directly."
  },
  {
    "objectID": "architecture.html#dashboards-apache-superset",
    "href": "architecture.html#dashboards-apache-superset",
    "title": "Architecture",
    "section": "2.4 Dashboards: Apache Superset",
    "text": "2.4 Dashboards: Apache Superset\nApache Superset is an open-source business intelligence product with comprehensive charting, dashboarding, and querying capabilities."
  },
  {
    "objectID": "architecture.html#notebook-environment-daskhub",
    "href": "architecture.html#notebook-environment-daskhub",
    "title": "Architecture",
    "section": "2.5 Notebook environment: Daskhub",
    "text": "2.5 Notebook environment: Daskhub\nDaskhub is a Helm chart to easily install JupyterHub and Dask Gateway for multiple users on a Kubernetes cluster.\n\n2.5.1 JupyterHub\nThe multi-tenant JupyterHub component creates on-demand, isolated pods for authenticated users, each with persistent storage for their R and Python notebook workspace.\n\n\n2.5.2 Dask Gateway\nDask Gateway allows easy utilization of a Dask cluster from notebook environments for distributed computation of massive datasets or parallelizable operations.\nReferences:\n\nJupyterhub - https://jupyterhub.readthedocs.io/en/stable/reference/index.html\nJupyterhub - kubernetes https://z2jh.jupyter.org/en/latest/index.html\nAuthentication for Jupyterhub https://oauthenticator.readthedocs.io/en/latest/index.html\nAWS Public Sector Blog article on Analyze terabyte-scale geospatial datasets with Dask and Jupyter on AWS"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Magasin (pronounced /ma.ɡa.zɛ̃/) is an end-to-end collection of open-source tools that are setup together to provide enterprise ready automated ingestion, transformation, and storage as well as the enabling infrastructure and tools to perform Data analysis / ML / AI and visualization of datasets."
  },
  {
    "objectID": "about.html#about-magasin",
    "href": "about.html#about-magasin",
    "title": "About",
    "section": "",
    "text": "Magasin (pronounced /ma.ɡa.zɛ̃/) is an end-to-end collection of open-source tools that are setup together to provide enterprise ready automated ingestion, transformation, and storage as well as the enabling infrastructure and tools to perform Data analysis / ML / AI and visualization of datasets."
  },
  {
    "objectID": "about.html#brief-history",
    "href": "about.html#brief-history",
    "title": "About",
    "section": "2 Brief history",
    "text": "2 Brief history\nMagasin grew out of Office of Innovation’s and Information and Communication Technology Division (ICTD) work to establish a RapidPro Data Warehouse to enable aggregation of global metrics across vendor instances.\nAlso informed by experiences of the Magic Box applied data science initiative, a range of components were evaluated and trialed with country offices to arrive at the current components and architecture.\nIn 2021, ICTD engaged one of its long term agreement vendors to assist with evolving the proof-of-concept into a minimum viable product (MVP).\nIn 2023, UNICEF started the journey to detach magasin from its organizational and cloud infrastructure dependencies, and release it as an open-source with the goal of becoming a Digital Public Good solution."
  },
  {
    "objectID": "docs-home.html",
    "href": "docs-home.html",
    "title": "Magasin documentation",
    "section": "",
    "text": "Main page"
  },
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "The scope of this privacy policy is related with the Magasin Documentation Site with the URL http://unicef.github.io/magasin (Site)\nThis privacy policy does not cover the GitHub repository (https://github.com/unicef/magasin) and other external pages related with magasin."
  },
  {
    "objectID": "privacy.html#scope",
    "href": "privacy.html#scope",
    "title": "Privacy Policy",
    "section": "",
    "text": "The scope of this privacy policy is related with the Magasin Documentation Site with the URL http://unicef.github.io/magasin (Site)\nThis privacy policy does not cover the GitHub repository (https://github.com/unicef/magasin) and other external pages related with magasin."
  },
  {
    "objectID": "privacy.html#purpose",
    "href": "privacy.html#purpose",
    "title": "Privacy Policy",
    "section": "2 Purpose",
    "text": "2 Purpose\nThe purpose of collecting the data is to create statistics about the number of visits, to improve the documentation and the product itself."
  },
  {
    "objectID": "privacy.html#personal-data-collected",
    "href": "privacy.html#personal-data-collected",
    "title": "Privacy Policy",
    "section": "3 Personal data collected",
    "text": "3 Personal data collected\nThe personal data that is collected is:\n\nIP address\nBrowser\nOperating system\nScreen size\nPages visited within the site\nTime spent in the site"
  },
  {
    "objectID": "privacy.html#cookies",
    "href": "privacy.html#cookies",
    "title": "Privacy Policy",
    "section": "4 Cookies",
    "text": "4 Cookies\nThe only cookie that the Site uses is for collecting the statistics."
  },
  {
    "objectID": "privacy.html#sharing-the-collected-data",
    "href": "privacy.html#sharing-the-collected-data",
    "title": "Privacy Policy",
    "section": "5 Sharing the collected data",
    "text": "5 Sharing the collected data\n\nThe personal data collected by UNICEF is kept in UNICEF’s servers and is not shared with any other third party.\nThe personal collected data is only accessible by internal magasin team which is composed by UNICEF staff members.\nUNICEF may share annonymized data for reporting purposes."
  },
  {
    "objectID": "privacy.html#third-party-hosting",
    "href": "privacy.html#third-party-hosting",
    "title": "Privacy Policy",
    "section": "6 Third party hosting",
    "text": "6 Third party hosting\nThe Site is hosted under GitHub Pages Service. Please visit https://docs.github.com/en/site-policy/privacy-policies/github-privacy-statement"
  },
  {
    "objectID": "install/uninstall.html",
    "href": "install/uninstall.html",
    "title": "Uninstall magasin",
    "section": "",
    "text": "You can uninstall magasin either by using the uninstall script script or manually."
  },
  {
    "objectID": "install/uninstall.html#uninstall-magasin-using-the-script-beta",
    "href": "install/uninstall.html#uninstall-magasin-using-the-script-beta",
    "title": "Uninstall magasin",
    "section": "0.1 Uninstall magasin using the script (beta)",
    "text": "0.1 Uninstall magasin using the script (beta)\nA simple way of getting rid of an instance of magasin within a kubernetes cluster is to use the uninstaller.\nFirst, ensure that kubectl is pointing to the target cluster. For instance, by running kubectl cluster-info. Take a look at this link if it is not pointing to the right cluster. Then, proceed to run the uninstaller:\n\nUninstall script for GNU/Linux:\ncurl -X https://unicef.github.io/magasin/uninstall-magasin.sh | bash\nUninstall script for MacOS:\ncurl -X https://unicef.github.io/magasin/uninstall-magasin.sh | zsh\nUninstall script for Windows:\nUse the manual uninstall."
  },
  {
    "objectID": "install/uninstall.html#advanced-use-of-uninstall-magasin.sh",
    "href": "install/uninstall.html#advanced-use-of-uninstall-magasin.sh",
    "title": "Uninstall magasin",
    "section": "0.2 Advanced use of uninstall-magasin.sh",
    "text": "0.2 Advanced use of uninstall-magasin.sh\nYou can obtain the list of options by adding the -h option\n./uninstall-magasin.sh -h\nUsage: uninstall-magasin.sh [-c] [-r realm_prefix-realm_postfix (magasin)] [-d] [-h]\n\nThis script uninstall all magasin components from a kubernetes cluster\n\nOptions:\n  -y  Skip prompting questions during uninstall.\n  -c  Only check if all pre-requisites are installed in the local machine.\n  -r  Realm prefix and suffix (default: magasin). Prefix and suffix are separated by '-'.\n        If more than one '-', the last one will be used as separator.\n        The realm 'magasin-new-dev' will set 'magasin-new' as prefix and 'dev' as suffix.\n  -d  Enable debug mode (displays all commands run).\n  -h  Display this help message and exit.\nExamples:\n\nOnly check if your computer has all the pre-requisites to run the uninstaller (namely kubectl and helm).\nuninstall-magasin.sh -c\nUninstall magasin from a realm that is different from the standard one\nuninstall-magasin.sh -r magasin-dev"
  },
  {
    "objectID": "install/manual-installation.html",
    "href": "install/manual-installation.html",
    "title": "Manual installation",
    "section": "",
    "text": "If for some reason the installation scripts do not work for you, or your system is not covered by them, you can install all requirements manually manually.\n\nInstall kubectl\nInstall helm\nInstall Python/pip\n\nEnsure kubectl is pointing to the right kubernetes cluster\nkubectl cluster-info\nAdd the magasin helm repo\nhelm repo add https://unicef.github.io/magasin/\nUpdate the magasin helm repo\nhelm repo update magasin\nInstall the different helm charts (dagster, drill, superset, daskhub)\n# helm install &lt;component&gt; magasin/&lt;component&gt; --namespace magasin-&lt;component&gt; --create-namespace\nhelm install dagster magasin/dagster --namespace magasin-dagster --create-namespace\nhelm install drill magasin/drill --namespace magasin-drill --create-namespace\nhelm install superset magasin/superset --namespace magasin-superset --create-namespace\nhelm install daskhub magasin/daskhub --namespace magasin-daskhub --create-namespace"
  },
  {
    "objectID": "install/setup-kubernetes.html",
    "href": "install/setup-kubernetes.html",
    "title": "Setup kubernetes",
    "section": "",
    "text": "Magasin components are designed for installation within Kubernetes clusters. Major cloud providers offer the ability to set up these clusters, but for exploration and testing, you can create one on your desktop.\nIf you already have a Kubernetes cluster, you may proceed without this step.\nFor setting up a Magasin instance in a local cluster on a desktop, it is advisable to have a minimum of 32 GB of RAM."
  },
  {
    "objectID": "install/setup-kubernetes.html#option-1-local-cluster-using-docker-desktop",
    "href": "install/setup-kubernetes.html#option-1-local-cluster-using-docker-desktop",
    "title": "Setup kubernetes",
    "section": "1.1 Option 1: Local cluster using Docker-desktop",
    "text": "1.1 Option 1: Local cluster using Docker-desktop\nIn case you do not have a Kubernetes cluster, another easy way to set it up is through Docker Desktop. It is available for:\n\nGNU/Linux\nMac OS X\nWindows\n\nOnce installed. In Settings / Kubernetes , enable Kubernetes. It will automatically install\n\n\n\nScreenshot of Docker Desktop Kubernetes Settings that allows to enable Kubernetes\n\n\nLastly, on a command line, create the new cluster and use it:\nkubectl config set-context magasin --namespace default --cluster docker-desktop --user=docker-desktop\nkubectl config use-context magasin\nTo ensure that the kubernetes cluster is the correct one check if the name corresponds to the\nkubectl get nodes\nNAME             STATUS   ROLES           AGE   VERSION\ndocker-desktop   Ready    control-plane   48m   v1.28.2\nkubectl get namespaces\nNAME              STATUS   AGE\ndefault           Active   49m\nkube-node-lease   Active   49m\nkube-public       Active   49m\nkube-system       Active   49m\nMore information: * https://docs.docker.com/desktop/kubernetes/ * https://birthday.play-with-docker.com/kubernetes-docker-desktop/"
  },
  {
    "objectID": "install/setup-kubernetes.html#option-2-local-cluster-using-minikube",
    "href": "install/setup-kubernetes.html#option-2-local-cluster-using-minikube",
    "title": "Setup kubernetes",
    "section": "1.2 Option 2: Local cluster using Minikube",
    "text": "1.2 Option 2: Local cluster using Minikube\nIf you don’t have a kubernetes cluster, for testing purposes, you can easily install minikube in your desktop. Minikube is a kubernetes cluster created for practicing and learning purposes.\nThe full installation details are described in https://minikube.sigs.k8s.io/docs/start/.\n# GNU/Linux Debian like\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb\nsudo dpkg -i minikube_latest_amd64.deb\n# Mac OS\nbrew install minikube\nOnce minikube is installed, please make sure to run:\nminikube start\nIf your system does not have the command kubectl already installed, it is also recommended to add the kubeclt alias\n\n\n\n\n\n\nNote\n\n\n\nNote that both of the two options above are just recommended for playing around without needing to deploy any infrastructure, but not for setting up an actual “shared” instance of magasin."
  },
  {
    "objectID": "end-user-guides.html",
    "href": "end-user-guides.html",
    "title": "End user guides",
    "section": "",
    "text": "Each of the open source components included in magasin has its onw end user documentation."
  },
  {
    "objectID": "end-user-guides.html#dagster",
    "href": "end-user-guides.html#dagster",
    "title": "End user guides",
    "section": "1 Dagster",
    "text": "1 Dagster\n\nGetting started\nDagster University"
  },
  {
    "objectID": "end-user-guides.html#jupyter-hub",
    "href": "end-user-guides.html#jupyter-hub",
    "title": "End user guides",
    "section": "2 Jupyter Hub",
    "text": "2 Jupyter Hub\n\nProject Jupyter Documentation"
  },
  {
    "objectID": "end-user-guides.html#apache-superset",
    "href": "end-user-guides.html#apache-superset",
    "title": "End user guides",
    "section": "3 Apache Superset",
    "text": "3 Apache Superset\n\nCreating your first dashboard\nExploring data in superset"
  },
  {
    "objectID": "security.html",
    "href": "security.html",
    "title": "Security",
    "section": "",
    "text": "Ensuring robust security measures within a Kubernetes cluster hosting diverse open-source applications is paramount in safeguarding sensitive data and maintaining operational integrity."
  },
  {
    "objectID": "security.html#reference-documentation",
    "href": "security.html#reference-documentation",
    "title": "Security",
    "section": "1 Reference documentation",
    "text": "1 Reference documentation\nEach component of magasin has its own level of security\n\nKubernetes Security\nApache Superset Security\nApache Drill Security\nApache Zookeeper"
  },
  {
    "objectID": "get-started/initial-analysis.html",
    "href": "get-started/initial-analysis.html",
    "title": "Step 1: Initial analysis",
    "section": "",
    "text": "This is the first out of the three steps of the magasin tutorial.\n\n1 Run jupyter notebooks.\n\nDownload the jupyter-notebook.\n\nDownload tutorial jupyter notebook.\nThe notebook is a file called dpg-insights.ipynb. We will use it later.\n\nLaunch Jupter Hub webpage on our cluster.\nRun the command:\nkubectl --namespace=magasin-daskhub get svc proxy-public \nThis will give you a table like this:\nNAME           TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nproxy-public   LoadBalancer   10.107.249.99   localhost     80:31035/TCP   44m\nThe EXTERNAL-IP value will give you the IP address in which the user interface of Jupyter hub is available. In our case, it is localhost.\nOpen a browser pointing to the external IP. In the example above http://localhost\nThis will display a login page\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that (1) the default installation does not use an encrypted connection between the client and the server and (2) anyone can launch the jupyter hub.\n\n\n\nIf you followed the default installation, just enter any username. For example: magasin.\nLoad the"
  },
  {
    "objectID": "get-started/index.html",
    "href": "get-started/index.html",
    "title": "Get started",
    "section": "",
    "text": "Magasin is an scalable end-to-end data platform based on open-source components that is natively run in a kubernetes cluster."
  },
  {
    "objectID": "get-started/index.html#local-kubernetes-cluster-for-testing",
    "href": "get-started/index.html#local-kubernetes-cluster-for-testing",
    "title": "Get started",
    "section": "1.1 Local kubernetes cluster for testing",
    "text": "1.1 Local kubernetes cluster for testing\nIn case you do not have a Kubernetes cluster, an way to set it up is through Docker Desktop. It is available for:\n\nGNU/Linux\nMac OS X\nWindows\n\nOnce installed. In Settings / Kubernetes , enable Kubernetes. It will automatically install everything required, including the command line utility kubectl.\n\n\n\nScreenshot of Docker Desktop Kubernetes Settings that allows to enable Kubernetes\n\n\nLastly, on a command line, create the new cluster and use it:\nkubectl config set-context magasin --namespace default --cluster docker-desktop --user=docker-desktop\nkubectl config use-context magasin\nTo ensure that the kubernetes cluster is the correct one check if the name corresponds to the\nkubectl get nodes\nNAME             STATUS   ROLES           AGE   VERSION\ndocker-desktop   Ready    control-plane   48m   v1.28.2\nkubectl get namespaces\nNAME              STATUS   AGE\ndefault           Active   49m\nkube-node-lease   Active   49m\nkube-public       Active   49m\nkube-system       Active   49m\nYou can also install minikube"
  },
  {
    "objectID": "get-started/index.html#check-everything-is-working",
    "href": "get-started/index.html#check-everything-is-working",
    "title": "Get started",
    "section": "2.1 Check everything is working",
    "text": "2.1 Check everything is working\nAfter running the setup you can check that all the pods in the magasin-* namespace are in status Running or Complete\nkubectl get pods --all-namespaces \nNAMESPACE          NAME                                                              READY   STATUS      RESTARTS        AGE\nkube-system        coredns-5dd5756b68-fj7bj                                          1/1     Running     0               7d\nkube-system        coredns-5dd5756b68-qbjf4                                          1/1     Running     0               7d\nkube-system        etcd-docker-desktop                                               1/1     Running     0               7d\nkube-system        kube-apiserver-docker-desktop                                     1/1     Running     0               7d\nkube-system        kube-controller-manager-docker-desktop                            1/1     Running     0               7d\nkube-system        kube-proxy-n8wwq                                                  1/1     Running     0               7d\nkube-system        kube-scheduler-docker-desktop                                     1/1     Running     0               7d\nkube-system        storage-provisioner                                               1/1     Running     0               7d\nkube-system        vpnkit-controller                                                 1/1     Running     0               7d\nmagasin-dagster    dagster-daemon-6f4fcbd697-z46xc                                   1/1     Running     0               6m24s\nmagasin-dagster    dagster-dagster-user-deployments-k8s-example-user-code-1-5qjrfh   1/1     Running     0               6m24s\nmagasin-dagster    dagster-dagster-webserver-788ddf4cf4-v8ndn                        1/1     Running     0               6m24s\nmagasin-dagster    dagster-postgresql-0                                              1/1     Running     0               6m24s\nmagasin-daskhub    api-daskhub-dask-gateway-6b6b84f47f-28pns                         1/1     Running     0               6m27s\nmagasin-daskhub    continuous-image-puller-9rv4q                                     1/1     Running     0               6m27s\nmagasin-daskhub    controller-daskhub-dask-gateway-c4d77dff9-rhgst                   1/1     Running     0               6m27s\nmagasin-daskhub    hub-6956d694bb-kl485                                              1/1     Running     0               6m27s\nmagasin-daskhub    proxy-666fbb4959-v99dj                                            1/1     Running     0               6m27s\nmagasin-daskhub    traefik-daskhub-dask-gateway-74965cbfbb-92m5f                     1/1     Running     0               6m27s\nmagasin-daskhub    user-scheduler-9b694f7d5-qvq4k                                    1/1     Running     0               6m27s\nmagasin-daskhub    user-scheduler-9b694f7d5-vbk69                                    1/1     Running     0               6m27s\nmagasin-drill      drillbit-0                                                        1/1     Running     1 (31s ago)     6m33s\nmagasin-drill      drillbit-1                                                        1/1     Running     0               6m33s\nmagasin-drill      zk-0                                                              1/1     Running     0               6m33s\nmagasin-superset   superset-7c88fcc74f-kjbw9                                         1/1     Running     0               6m23s\nmagasin-superset   superset-init-db-q8tc8                                            0/1     Completed   0               6m23s\nmagasin-superset   superset-postgresql-0                                             1/1     Running     0               6m23s\nmagasin-superset   superset-redis-master-0                                           1/1     Running     0               6m23s\nmagasin-superset   superset-worker-df94c5947-lsvsl                                   1/1     Running     1 (2m44s ago)   6m23s\n\nNote that the default installation is fine for testing purposes, but for a production environment you should follow the production deployment guides"
  },
  {
    "objectID": "get-started/index.html#next-steps",
    "href": "get-started/index.html#next-steps",
    "title": "Get started",
    "section": "2.2 Next steps",
    "text": "2.2 Next steps\nOk, now you have a fully running instance of magasin in your kubernetes cluster, so what now:\n\n[Start using magasin]\nMagasin architecture. Understand how magasin works."
  },
  {
    "objectID": "get-started/automate-data-ingestion.html",
    "href": "get-started/automate-data-ingestion.html",
    "title": "Step 2: Automate data ingestion",
    "section": "",
    "text": "Now let’s automate our jupter notebook.\n\nCreate a project"
  },
  {
    "objectID": "admin-guides/superset.html",
    "href": "admin-guides/superset.html",
    "title": "Superset guide",
    "section": "",
    "text": "A quick guide for an magasin administrator on superset."
  },
  {
    "objectID": "admin-guides/superset.html#backup-the-superset-database-into-the-local-filesystem",
    "href": "admin-guides/superset.html#backup-the-superset-database-into-the-local-filesystem",
    "title": "Superset guide",
    "section": "1 Backup the superset database into the local filesystem",
    "text": "1 Backup the superset database into the local filesystem\nA dump (backup) of the database may be useful when a new version of superset is going to be installed, or to keep a regular scheduled backup of the superset data.\nTo create a dump of the database, you can use the script scripts/superset/dump-superset-db.sh:\n./dump-superset-db.sh -n magasin-superset-prd`\nWhere the option -n is to specify the namespace of the superset instance (defaults to magasin-superset).\nThis script will request request the password of the superset user, to perform the dump. You can get the password from the secret superset-env within the same namespace.\nThe output of running this script is the file superset_dump.sql in the current working folder. Note that this script assumes that the name of the database is superset.\nAlternatively, these are the manual steps to do the same:\n\nLaunch a terminal in the posgresql pod (replace the namespace if required)\n kubectl exec superset-postgresql-0 -ti --namespace magasin-superset -- /bin/bash\nWithin the pod shell, extract and compress the data. The below command will ask for the superset database password (you can find it in the superset-env secret, defaults to superset)\npg_dump -U superset -d superset &gt; /tmp/superset_dump.sql\n# where -U is the username of the database, and -d is the database name.\n\ntar -C /tmp -czvf /tmp/supserset_dump.tgz supserset_dump.sql\n# where -C is to use /tmp as base folder -c collect, z zip, v=verboze and f output file\nExit the pod shell and copy the dump file into /tmp in the local filesystem of the pod. exit\n# Exit the pod shell\nexit\n\n# Copy the tgz to the local environment.\nkubectl cp magasin-superset/superset-postgresql-0:/tmp/superset_dump.tgz \\\n./supserset_dump.tgz --retries 100\n\n# Syntax: kubectl cp namespace/pod:path local-path. \n# The option --retries is to fix potential network issues\nLastly, delete the db dump from the postgres pod\nkubectl exec magasin-superset/superset-postgresql-0 -- \\\nrm /tmp/supeset_dump.tgz /tmp/superset_dump.sql  \n\n\n1.1 Restore superset database from local filesystem\nTo restore the database from the previous step.\n\nCopy the database to the postgres pod.\nkubectl cp superset_dump.sql magasin-superset/superset-postgresql-0:/tmp/superset_dump.sql\nLaunch the shell\nkubectl exec superset-postgresql-0 -ti --namespace magasin-superset -- /bin/bash\nIf the database exist, you may need to drop it first\npsql --username postgres\nThe password can be found in superset-postgresql secret postgres-password\nIf you get an error indicating the database is being used run the command:\n\nSELECT pg_terminate_backend(pg_stat_activity.pid) \n    FROM pg_stat_activity \n    WHERE pg_stat_activity.datname = 'superset' AND pid &lt;&gt; pg_backend_pid();\n\n-- To delete the superset database: (don't forget the ; at the end)\nDROP DATABASE superset;\n\n--- Then create an empty database (don't forget the ; at the end)\nCREATE DATABASE superset;\n\n-- Then quit psql\n\\q\nFinally, restore the database\npsql --username superset --dbname superset -f /tmp/superset_dump.sql\nOr alternatively less verbose option\npsql -U superset -d superset2 -f ./tmp/superset_dump.sql\n\nNow, you can restart the superset pod. First find the superset-xxxxxxxxx-xxxxx and delete it\nkubectl get pods --namespace magasin-superset\n\nNAME                               READY   STATUS      RESTARTS   AGE\nsuperset-75b79c6c8d-jttd9          1/1     Running     0          62m\nsuperset-init-db-bhc25             0/1     Completed   0          62m\nsuperset-postgresql-0              1/1     Running     0          62m\nsuperset-redis-master-0            1/1     Running     0          62m\nsuperset-worker-85dfbb48dd-6bhjp   1/1     Running     0          62m\nkubectl delete pod superset-75b79c6c8d-jttd9"
  },
  {
    "objectID": "admin-guides/superset.html#postgresql",
    "href": "admin-guides/superset.html#postgresql",
    "title": "Superset guide",
    "section": "2 PostgreSQL",
    "text": "2 PostgreSQL\n\nGet list of databases\nSELECT datname FROM pg_database WHERE datistemplate = false;\nGet the user owner of database ‘database_name’\nSELECT d.datname as \"Name\",\npg_catalog.pg_get_userbyid(d.datdba) as \"Owner\"\nFROM pg_catalog.pg_database d\nWHERE d.datname = 'database_name'\nORDER BY 1;"
  },
  {
    "objectID": "admin-guides/superset.html#troubleshooting",
    "href": "admin-guides/superset.html#troubleshooting",
    "title": "Superset guide",
    "section": "3 Troubleshooting",
    "text": "3 Troubleshooting\n\nGet the events of a pod\nkubectl describe pod superset-564564dd4-2lzb6 --namespace magasin-superset\nGet logs of the containers\nkubectl logs &lt;podname&gt; -f --namespace &lt;namespace&gt; \nExample:\nkubectl logs superset-564564dd4-2lzb6 -f --namespace magasin-superset\nwhere -f continues displaying the new logs.\nWith --all-containers you can see the logs of all the containers instead of the one that is being executed. This is useful when the current container of the pod gets stuck because of the previous container output was not the expected. Example:\nkubectl logs superset-564564dd4-2lzb6 --namespace magasin-superset --all-containers"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "The purpose of this document is to outline the workflow and guidelines for code contributing to this repository."
  },
  {
    "objectID": "contributing.html#purpose",
    "href": "contributing.html#purpose",
    "title": "Contributing",
    "section": "",
    "text": "The purpose of this document is to outline the workflow and guidelines for code contributing to this repository."
  },
  {
    "objectID": "contributing.html#principles",
    "href": "contributing.html#principles",
    "title": "Contributing",
    "section": "0.2 Principles",
    "text": "0.2 Principles\nOur principles are the roots, core and foundation. All our standardization decisions are based on the following principles:\n\nEnhance the developer experience. Standards ultimate goal is to create a pleasant development experience.\nProvide management key information. Workflows should enable management to tackle inefficiencies, pain points and increase output.\nSeek for simplicity. If there are different options, we should aim for the simplest one.\n\nIn order to achieve these principles, Github is the main tool we use to track and manage development-related work, from requirements gathering to production release."
  },
  {
    "objectID": "contributing.html#authors",
    "href": "contributing.html#authors",
    "title": "Contributing",
    "section": "0.3 Authors",
    "text": "0.3 Authors\nThis document was created by Juan Merlos (@merlos)"
  },
  {
    "objectID": "contributing.html#branch-strategy",
    "href": "contributing.html#branch-strategy",
    "title": "Contributing",
    "section": "1.1 Branch strategy",
    "text": "1.1 Branch strategy\nBranching in each repository will follow a trunk-based model. This model involves one eternal “trunk” called master or main from which all other branches originate.\nAll development work will take place on these branches and will be committed to master via pull requests with mandatory approval/review.\nThe goal of this approach is to have a highly stable codebase that is releasable on demand at all times.\nThe additional branches will be created for each feature, release, or hotfix. Each of these branches will have specific purposes and follow strict rules as to how they should be used. At a high level, the branching rules are described as follows:\n\n\n\nBranch type\nBranch from\nMerge to\nNaming convention\nExample\n\n\n\n\nfeature\nmain\nmain\nfeature/&lt;desc&gt;\nfeature/my-feature\n\n\nfix\nmain\nmain\nfeature/&lt;desc&gt;\nfeature/my-feature\n\n\nrelease\nmain\n-\nv&lt;semver&gt;\nv1.0.0\n\n\nhotfix\nmain\nmain, release\nhotfix/&lt;desc&gt;\nhotfix/my-fix\n\n\n\nThe purpose and correct usage of each supporting branch is elaborated in the following sub-sections. The below graphic can be used as an example of a complete workflow involving all branch types.\n\n\n\nBranch strategy flow\n\n\n\n1.1.1 Feature branches\nFor each new feature, or for updates to a given feature, a separate branch will be created. Feature branches must originate from the master branch. Since no development takes place on the master branch, feature branches are expected to be the most common type of branch.\nHowever, these branches should be short-lived. They will exist only as long as the feature is under development and never longer than a sprint. Ideally, a feature branch should involve one developer over a few (1-3) days of work.\nProduct backlog items (issues) should be defined with this in mind — if work on a feature is taking too long, then the item needs to be scoped differently (see the section on Delivery).\nOnce a feature branch is merged to master via pull request, it should be deleted. Any future work, even if it is related, will require a new branch.\nWhenever a feature branch is created, it should be related to an existing issue. Whereas issues will be tagged with the classification (bug, enhancement, etc), the naming of the feature branches is more free.\n\n\n1.1.2 Release branches\nRelease branches will be created for each version of the source that is destined for production release. This enables all preparation work for an upcoming release to take place even while development work for future releases is committed to the master.\nThe creation of a release branch will always correspond to an upcoming deployment. Since the master branch has a high standard of stability, it should be rare that further development work is needed to refine a release for production. If such a need arises, it will follow the hotfix strategy defined below.\nRelease branches never merge back into the trunk. A release branch should be representative of a release at a given point in time. There is no mandatory end of life for a release branch. These branches can exist indefinitely, as marker in release history and for potential rollback should the need arise.\nRelease branches follow the semantic version naming convention (i.e 1.0.0, 1.1.0). When a release branch is created it is also tagged with the same name.\n\n\n1.1.3 Hotfix branches\nHotfixes are small changes that need to be released to production more quickly (e.g. due to the severity of the issue). As with release and feature branches, they should always originate from master.\nUpon completion of the work, a hotfix branch will be merged into master. If the hotfix is also required in a release branch to be brought to production immediately, the change can be cherry-picked upon being merged into master. If no current release branch exists, one should be created for this purpose.\nOnce the corresponding pull requests complete, the temporary hotfix branch should be removed.\nWhenever there is a hotfix merged into a release branch, the commit that includes the fix shall be tagged updating the PATCH number, for instance, changing from 1.0.0 to 1.0.1.\nThis approach allows to provide long term support to a particular version."
  },
  {
    "objectID": "contributing.html#pull-requests-and-commits-pr",
    "href": "contributing.html#pull-requests-and-commits-pr",
    "title": "Contributing",
    "section": "1.2 Pull requests and commits (PR)",
    "text": "1.2 Pull requests and commits (PR)\nThe merging of supporting branches (feature and hotfix) into the master branch will always require a pull request.\nPull requests should include a reference to the issue numbers either in the commit or in the description of the PR. in the commit comment, using the following convention (where 0000 is the issue number and &lt;comments&gt; is placeholder for any optional comments).\n&lt;comments&gt; #0000 &lt;comments&gt; \nUsing comments like this will ensure that the pull request is automatically linked to all related issues.\nWhen submitting a pull request ensure:\n\nWell-formatted code*: Ensure that the committed code adheres to established coding standards and is well-formatted. Consistent indentation, proper naming conventions, and clean formatting improve code readability and maintainability.\nNo debugging or temporary code: Avoid including debugging statements, temporary code, or commented-out blocks. These can clutter the codebase and make it harder for others to understand and maintain the code.\nAvoid unrelated changes: The PR should focus on a specificfeature. Avoid including unrelated changes in a single commit. If you have multiple unrelated changes, consider creating separate PR for each change.\nNo Sensitive information: Ensure that commits do not include any sensitive information like passwords, API keys, or personal data. Such information should be securely managed outside of the code repository.\nConsistency: Maintain a consistent style and structure throughout the project. Consistency helps establish good coding practices and makes it easier for the team to understand and collaborate on the codebase.\nDocumentation update: Ensure that the code, scripts, documentation and README files are up to date.\n\nCommit messages shall help the reviewer to understand what are the changes that have been introduced and the should tell the reviewer what changes have been introduced. These are properties that commits shall have:\n\nAtomicity: Commits should be atomic, meaning they should represent a single logical change. It’s best to keep each commit focused on a specific task within the PR. This allows for easier code review, debugging, and reverting if necessary.\nClarity and descriptive messages: Commit messages should be clear, descriptive, and concise. They should summarize the purpose and content of the commit. A good commit message helps others understand the changes at a glance and provides context for future reference.\nExample. Too generic not descriptive message ``` ❌ Avoid Updates Readme\n✅ Better Add CONTRIBUTING link to Readme ```\nLogical progression: Commits should follow a logical progression, building upon previous commits. Each commit should leave the codebase in a stable and working state, ensuring that others can pull changes without introducing errors.\n\n\n1.2.1 Approvals\nCompletion of pull requests on main be generally restricted to designated approvers. The job of the approvers is to diligently review all code being merged to master to ensure the utmost stability of that branch.\nReviewers shall ensure the quality of the code, style and documentation. They shall ensure that it follows the practices of the commits and pull requests exposed on this document.\nTheir aim should be to maintain a release-ready main branch at all times."
  }
]