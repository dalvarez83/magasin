---
title: "Step 2: Automate data ingestion"
---

This is the second step of the [magasin getting started tutorial](tutorial-overview.qmd). In this step we will see how to automate the data ingestion.

In the [previous step](./exploratory-analysis.qmd), we delved into the DPG Alliance API data, generating graphs and uncovering insights along the way. 

## Background 
Typically, after identifying intriguing insights, it's common as next step to periodically update the data to monitor the evolution of the data.

Automating this process is highly advantageous as it eliminates the need for repetitive tasks. In our scenario, the workflow involves fetching data from the DPG API, followed by cleaning and processing it for seamless integration with a business intelligence dashboard creation tool, Superset, that will be used in the [next step](./create-a-dashboard.qmd).

The good news is that we have already completed the heavy lifting using the Jupyter Notebook. This is advantageous because the code we will be writing is essentially the same; we just need to make some tweaks to adapt it into a [Dagster pipeline](https://dagster.io).

### Advantages of using Dagster

[Dagster](https://dagster.io) is what is known as a pipeline orchestrator, which essentially helps us manage the ingestion of data from multiple sources. 

The benefits of using a framework like Dagster are manifold. It allows us to approach tasks in a structured manner, facilitating scalability and monitoring. Anyone who has dealt with gathering data from multiple sources will attest that it can quickly turn into a nightmare if not managed properly. Dagster provides an excellent starting point for tackling such challenges.

What usually happends when you start automating gathering data, processing and mixing data from multiple data sources is that:

1) It begins simple but with time it becomes more complex. You add more and more data sources, more logic to clean the data, stablish more dependencies between the data sources, etc. If this is not done properly you end up with a mess. Dagster provides us a framework that helps to write our code in a more structured way, as well as tools to debug and monitor the ingestion of the data.

2) It eventually may break. Data sources may change with time and with that your pipelines break. For instance, DPGA may change the API unexpectedly, and with that our automation will fail too. Dagster allows you to monitor failures as well as to debug where it failed. 

Another advantage of Dagster, it that it uses Python as programming language, so, as we said earlier, most of the work we did for the Jupyter Notebook can be reused.

### Storing data as parquet files

In the magasin architecture, as general approach, we stand to store data assets as files. In particular, we recommend the use of [Apache parquet file format](https://parquet.apache.org/). 

The main reason to use files is:

1. First, because it is an economic way to store data. Storage services in the cloud or in premises is relatively cheap. 

2. Second, because it does provide more flexibility when changes on the underlying structure are introduced, at least compared with setting up a SQL database downstream. 

3. In addition, it allows also to easily store more types of data such as documents or images. Lastly, in terms of governance and sharing the datasets, the problem is simplified to setting up file sharing permissions.  



### Parquet format for structured data.

Parquet is a columnar storage file format that is commonly used in the Big Data ecosystem. It is designed to efficiently store and process large amounts of data. 

Using Parquet to store processed datasets is beneficial because it optimizes storage, improves query performance, and enhances the overall efficiency of data processing. Its compatibility with popular Big Data processing frameworks and support for schema evolution make it a versatile choice for storing large-scale, evolving datasets in data lakes and data warehouses.

### MinIO, magasin's storage layer (optional)

For keeping a consistent approach across the different cloud providers, magasin includes a component called  [MinIO](http://min.io). It gives an [Amazon S3](https://en.wikipedia.org/wiki/Amazon_S3) compatible file (object) store. 

Thanks to MinIO, regardless of what infrastructure you're using, your pipelines will work the same, you do not need to adapt how you store the data if you move to another provider. 

Whereas to maintain a vendor-agnostic architecture we leverage MinIO. You have the flexibility to bypass this component if you find alternative methods more suitable for storing your ingested data. This decision may be based on better alignment with your existing processes or specific use case requirements.

For instance, instead of MinIO, you can directly utilize a vendor-specific object store like Azure Blobs, Amazon S3, or Google Cloud Storage. Alternatively, in contrast to the file-storage approach of the datasets you may choose to write your data directly into a database, such as DuckDB or PostgreSQL.

For the purposes of this tutorial we will be using MinIO. And the first step is to setup a bucket. A bucket is somewhat similar to a folder.

So, let's start. 

## Create a bucket in MinIO

The first thing we need to do is to setup where we are going to store our data. Till now, we've been storing the data in our Jupyter lab space, but we need a place where we can securely save our datasets.

In one command line shell launch the minio API

```sh
mag minio api
```

Set the [minio client](https://min.io/docs/minio/linux/reference/minio-mc.html) alias. 

```sh
mc alias set myminio http://localhost:9000 minio minio123
# mc alias set <alias-name> <endpoint> <access-key> <secret-key>
```

Where `mc` is a tool similar to `mag`, but specific for minio. It is installed during the magasin installation.

Once we have configured the alias, we just need to create the bucket:
```sh
 mag minio add bucket --bucket-name magasin 
```

Note that the default alias that `mag` asumes is `myminio`. If you used another alias you can use `--alias` option: `mag minio add bucket --bucket-name magasin --alias myalias`.

::: {.callout-tip}
## Tip: mag shortcuts
`mag` allows you to shorten the commands using the alias. For example:
`mag minio add bucket --bucket-name magasin --alias myalias` can be written as `mag m a b -b magasin -a myalias`. Using the shorten version is a way of speeding up your interaction with the command, but it is less readable. 

When you type `mag --help` or `mag <command> --help` you can see the shortcut versions in parenthesis

For example for `mag --help`, you can see that daskhub shortcut is `dh` and minio is `m`
```sh
Usage: mag [OPTIONS] COMMAND [ARGS]...
...
Commands:
  dagster (d)    Dagster commands
  daskhub (dh)   Daskhub/Jupyterhub commands
  drill (dr)     Apache Drill commands
  minio (m)      MinIO commands
  superset (ss)  Apache Superset commands
```
:::

Now, we have a bucket in MinIO that allows us to store files.

## Create a dagster pipeline 

So, the next step is to create a pipeline. A pipeline is just a piece of code that moves data from place to another and that can introduce some changes before saving it in the destination place. In our case the pipeline will take the data from the DPGA API and store it in a MinIO bucket.

The first thing we need to do is to install Dagster.

```sh
pip install dagster==1.6.0 dagster-webserver==1.6.0
```

:::{.callout-note}
Dagster is a very agile product that is continuously evolving, this means that you have to be cognizant of the version you're running. 
   
 You can check the version installed in your cluster by running `helm list --all-namespaces` and looking at the `APP VERSION` column.
 
 Then run pip install `pip install dagster==<version>`
:::


### Add the pipeline code 

Once Dagster is installed, we're going to create a new project using the default structure prodivded by Dagster. This should be the default procedure for creating any new pipeline.

```sh
dagster project scaffold --name dpga-pipeline
```
```sh
Creating a Dagster project at /home/magasin/dpga-pipeline.
Creating a Dagster code location at /home/magasin/dpga-pipeline.
Generated files for Dagster code location in /home/magasin/dpga-pipeline.
Generated files for Dagster project in /home/magasin/dpga-pipeline.
Success! Created dpga-pipeline at /home/magasin/dpga-pipeline.
```

By scaffolding our project, Dagster creates a basic structure of a python package that could be installed using `pip` as any other package as well as some additional metadata files that will 
be used by Dagster to run the pipeline. You ahve some more info in the [Dagster documentation](https://docs.dagster.io/guides/understanding-dagster-project-files).

Now, lets add our code. Open the file `dpga-pipeline/dpga_pipeline/assets.py`

```{.python filename="dpga-pipeline/dpga_pipeline/assets.py"}
import requests
import pandas as pd
from pandas import DataFrame

from dagster import asset

@asset
def raw_dpgs() -> DataFrame:
  """ DPGs data from the API"""
  dpgs_json_dict = requests.get("https://api.digitalpublicgoods.net/dpgs").json()
  df = pd.DataFrame.from_dict(dpgs_json_dict)
  return df

@asset
def deployment_countries(raw_dpgs: DataFrame) -> DataFrame:
  df = raw_dpgs
  df_loc = pd.merge(df, pd.json_normalize(df["locations"]), left_index=True, right_index=True)
  df_deployment_countries = df_loc.explode("deploymentCountries")
  df_deployment_countries[["name","deploymentCountries"]]

  return df_deployment_countries
```

As you can see the code seems pretty similar to what we wrote in our [exploratory analysis](exploratory-analysis.qmd). 

The in the code we have defined two `@assets`. An asset according to the Dagster definition is:

> An asset is an object in persistent storage, such as a table, file, or persisted machine learning model. A Software-defined Asset is a Dagster object that couples an asset to the function and upstream assets used to produce its contents.
 
In our case, `raw_dpgs`, stores the dpgs as they come from the API as a DataFrame, and `deployment_countries` that extracts the one row per country in which the DPG has been deplayed.

Another thing that you can notice in the code is that in the definition of the deployment_countries asset, we are passing `raw_dpgs: DataFrame`. That will tell Dagster that deployment_countries depends on the `raw_dpgs` and it will be used as input.

As you noticed, we are using a couple of packages that need to be installed `pandas` and `requests`. To install them,  in `dpga-pipeline/setup.py` we add them in the `install_requires` array.

```{.python filename="dagster-pipeline/setup.py"}
setup(
  # ...
  install_requires=[
        "dagster",
        "dagster-cloud",
        "pandas",       # <--- Add this line 
        "requests"      # <---- Add this line too
    ],
  #...
)
```

Ok, So now let's test if this is working so far. To do that we will first install the pipeline package in [editable mode](https://pip.pypa.io/en/latest/topics/local-project-installs/#editable-installs) (`-e`). This allows you to edit the package without needing to install it again.

```sh
pip install -e '.[dev]'
```

Then, we will launch the Dagster user interface:
```sh 
dagster dev
```

This launches a server in port 3000 on localhost. So just open `http://localhost:3000`

You should see something like:

![Dagster user interface](../images/get-started/dagster-ui-unmaterialized.png)

#### Save the assets in MinIO.

Till now, we've been working on the development machine file system. The next step is to save the information we want to keep in MinIO. 

To access the MinIO bucket we will use [fsspec](https://filesystem-spec.readthedocs.io/en/latest/). This python library provides an standard interface regardless of the underlying filesystem. So, if you chose to use other file system to run this example, you can just change the environment variables and the  address. 

MinIO provides an S3 compatible bucket file system, so we will use it.
First we will add the dependencies `fsspec` and `s3fs`. 

```{.python filename="dagster-pipeline/setup.py"}
setup(
   #...
    install_requires=[
        "dagster",
        "dagster-cloud",
        "pandas",
        "requests",
        "fsspec",   # <---- New dependency
        "s3fs"      # <---- New dependency
    ],
    #...
)

```

Now, we're going to modify our assets to use the minio filesystem.

```{.python filename="dpga-pipeline/dpga_pipeline/assets.py"}
import fsspec
import requests
import pandas as pd
from pandas import DataFrame
from dagster import asset

@asset
def raw_dpgs() -> DataFrame:
  """ DPGs data from the API"""
  # Load from API
  dpgs_json_dict = requests.get("https://api.digitalpublicgoods.net/dpgs").json()  

  # Convert to pandas dataframe
  df = pd.DataFrame.from_dict(dpgs_json_dict)
  return df

@asset
def deployment_countries(raw_dpgs: DataFrame) -> DataFrame:
   
  df = raw_dpgs
  df_loc = pd.merge(df, pd.json_normalize(df["locations"]), left_index=True, right_index=True)
  df_deployment_countries = df_loc.explode("deploymentCountries")
  df_deployment_countries = df_deployment_countries[["id", "name","deploymentCountries"]]
  
  # Save to MinIO
  fs= fsspec.filesystem('s3')
  with fs.open('/magasin/data/deployment_countries.parquet','wb') as f:
    df_deployment_countries.to_parquet(f)
    
  return df_deployment_countries
```

Then, we will setup some environment variables that will setup the Minio S3 bucket credentials. Add the `.env` file in the root of your project (same folder as `setup.py`). 

```sh
FSSPEC_S3_ENDPOINT_URL='http://localhost:9000'
FSSPEC_S3_KEY='minio'
FSSPEC_S3_SECRET='minio123'
```

As you can see we are indicating in the `.env` file that the endpoint of our minio is in localhost port 9000. To enable this service we need to run the following command 

```sh
mag minio api
```

While this command is running it will forward any connection to the port 9000 to the our minio instance in the kubernetes cluster. You shoud keep running during this till you are instructed to do close it.

In another window, we need to reinstall the pipeline so the new dependencies are loaded, and, then, we can run Dagster:

```sh

pip install -e '.[dev]'
dagster dev
```

Note that after you launch `dagster dev` you should see something like:

```
dagster - INFO - Loaded environment variables from .env file: FSSPEC_S3_ENDPOINT_URL,FSSPEC_S3_KEY,FSSPEC_S3_SECRET
```

This is because Dagster loads all the .env file automatically and exposes the variables to the code.

Open again the browser pointing to `http://localhost:3000` and in the dagster UI and run `Materialize all`. 

This time, all files should have been materialized in the `magasin` bucket.

To test if the files are there. In a terminal run:

```sh
mc ls myminio/magasin/data
```


### Adding a job scheduler

Until now, we have been materializing manually our assets. However, automating this task is indeed the ultimate goal of setting up a pipeline.

In Dagster, you have available [schedulers](https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules) which basically run your pipeline, or pieces of it, in a fixed interval. Dagster schedulers follow a [cron style format](https://en.wikipedia.org/wiki/Cron).

```{.python filename="dpga-pipeline/dpga_pipeline/assets.py"}
#__init__.py
from dagster import Definitions, load_assets_from_modules, define_asset_job, ScheduleDefinition
from . import assets

all_assets = load_assets_from_modules([assets])

# Create an asset job that materializes all assets of the pipeline
all_assets_job = define_asset_job(name="all_assets_job",
                                  selection=all_assets,
                                  description="Gets all the DPG assets")
# Create a scheduler
main_schedule = ScheduleDefinition(job=all_assets_job,
                                   cron_schedule="* * * * *"
                                   )

defs = Definitions(
    assets=all_assets,
    jobs=[all_assets_job],
    schedules=[main_schedule]
)
```

What we did in the code above is to:

1. Add a `job`. A job, is basically a selection of assets that will be materialized together in the same run.

2. Define a schedule. The schedule will launch the job at specified time intervals. In our case every minute (`* * * * *`). 

::: {.callout-tip}
## Tip: Understanding cron jobs
The job cron format is used to specify the schedule for recurring tasks or jobs in Unix-like operating systems and cron job scheduling systems. It consists of **five fields separated by spaces**, representing different aspects of the schedule: 

  ```
  <minute> <hour> <day-of-month> <month> <day-of-week>
  ```

* Minute (0-59): Specifies the minute of the hour when the job should run. Valid values range from 0 to 59.

* Hour (0-23): Specifies the hour of the day when the job should run. Valid values range from 0 to 23, where 0 represents midnight and 23 represents 11 PM.

* Day of Month (1-31): Specifies the day of the month when the job should run. Valid values range from 1 to 31, depending on the month.

* Month (1-12): Specifies the month of the year when the job should run. Valid values range from 1 to 12, where 1 represents January and 12 represents December.

* Day of Week (0-7): Specifies the day of the week when the job should run. Both 0 and 7 represent Sunday, while 1 represents Monday, and so on, up to 6 representing Saturday.

Each field can contain a single value, a list of values separated by commas, a range of values specified with a hyphen, or an asterisk (*) to indicate all possible values. Additionally, you can use special characters such as slashes (/) for specifying intervals and question marks (?) for leaving a field unspecified (e.g., for day of month or day of week when the other field should match).

Here you have some [examples of cron intervals](https://crontab.guru/examples.html)

| Cron Expression | Description                                         |
|-----------------|-----------------------------------------------------|
| `0 0 * * *`     | Run a task every day at midnight (00:00).           |
| `15 2 * * *`    | Run a task at 2:15 AM every day.                    |
| `0 0 * * 1`     | Run a task every Monday at midnight (00:00).        |
| `0 12 * * 1-5`  | Run a task every weekday (Monday to Friday) at 12 PM (noon). |
| `*/15 * * * *`  | Run a task every 15 minutes.                       |
| `0 */2 * * *`   | Run a task every 2 hours, starting from midnight.  |
| `30 3 * * 6`    | Run a task every Saturday at 3:30 AM.               |
| `0 0 1 * *`     | Run a task at midnight on the first day of every month. |
| `0 0 1 1 *`     | Run a task at midnight on January 1st every year.   |

:::

If you launch again `dagster dev` and you go to Overview -> Jobs, you can enable the job.

![Scheduled job](../images/get-started/dagster-jobs.png)


### Deploy the pipeline in the cluster

Till now we have been working on a development environment on our own computer. However, we should launch our pipeline within our kubernetes cluster.

To do that we will deploy a container (pod) in our cluster that Dagster will use to run our pipeline.


```

```

Now we have to update our kubernetes deployment to include this new pipeline (a.k.a. code location in Dagster terminology)

```
helm upgrade dagster --namespace magasin-dagster -f code-dagster.yaml
```

To open the Dagster user interface of the instance running in our Kubernetes cluster we need to run

```sh
mag dagster ui
```

To check 


## Summary

## What's next

[Go to next step, create a dashboard in Superset](./create-a-dashboard.qmd)

Dagster has a learning curve, and it requires some work in order to get used to it, but the documentation is fairly good.

Here you have a few links to get started with dagster:

1. [Get started with dagster](https://docs.dagster.io/getting-started)
2.. [Dagster Essentials](https://courses.dagster.io/courses/dagster-essentials)
